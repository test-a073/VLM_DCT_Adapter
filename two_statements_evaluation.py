import os
import json
import logging
import yaml
import re
from typing import Dict, List, Optional, Any

# Attempt to import openai, require manual installation if not found
import openai
from datasets import Dataset
from evaluator.generic_evaluator import GenericLLMEvaluator

# Configuration for logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Try to use mmengine.ConfigDict, with a fallback to a basic dict wrapper
try:
    from mmengine.config import ConfigDict
except ImportError:
    logging.warning("mmengine.config.ConfigDict not found. Using a basic dict wrapper. Consider installing mmengine.")
    class ConfigDict(dict):
        def __getattr__(self, name):
            try:
                return self[name]
            except KeyError:
                raise AttributeError(name)

        def __setattr__(self, name, value):
            self[name] = value

# Helper function to load OpenAI configuration
def load_openai_config(config_path: str) -> dict:
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    logger.warning(f"OpenAI config file not found at {config_path}. Returning empty dict.")
    return {}

# Prompt template for evaluating two statements
TWO_STATEMENT_PROMPT_TEMPLATE = {
    "template": """[Instruction]
You are an impartial judge. You will be given a user's query and two responses (Response A and Response B) generated by AI assistants.
Your task is to evaluate each response individually based on the query, considering helpfulness, relevance, accuracy, depth, creativity, and level of detail.
Provide a separate evaluation (a short explanation) and a score (on a scale of 1 to 10) for each response.

Please structure your output strictly as follows:

Evaluation for Response A:
[Your detailed evaluation of Response A]
Score for Response A: [Score for A, e.g., 7/10 or 7]

Evaluation for Response B:
[Your detailed evaluation of Response B]
Score for Response B: [Score for B, e.g., 9/10 or 9]

Ensure your output strictly follows this format. Do not add any other text before or after this structured evaluation.

[User Query]
{query}

[Response A]
{response_A}

[Response B]
{response_B}
""",
    "input_columns": ["query", "response_A", "response_B"], # Columns expected from the input data
}

# Post-processor for the judge's output when evaluating two statements
def two_statement_postprocessor(judge_response_text: str) -> Dict[str, Any]:
    """
    Parses the judge's raw text output to extract evaluations and scores for two responses.
    """
    output = {
        "response_A": {"score": None, "explanation": None},
        "response_B": {"score": None, "explanation": None},
        "raw_judge_response": judge_response_text,
        "parsing_errors": []  # To store specific parsing issues
    }

    try:
        # Explanation for Response A
        exp_a_match = re.search(
            r"Evaluation for Response A:\s*(.*?)\s*Score for Response A:",
            judge_response_text, re.DOTALL | re.IGNORECASE
        )
        if exp_a_match:
            output["response_A"]["explanation"] = exp_a_match.group(1).strip()
        else:
            output["parsing_errors"].append("Could not parse explanation for Response A.")

        # Score for Response A
        score_a_match = re.search(
            r"Score for Response A:\s*(\d+(?:\.\d+)?)(?:/\d+)?", # Matches "7" or "7.5" or "7/10"
            judge_response_text, re.IGNORECASE
        )
        if score_a_match:
            try:
                output["response_A"]["score"] = float(score_a_match.group(1))
            except ValueError:
                output["parsing_errors"].append(f"Failed to parse score for Response A from '{score_a_match.group(1)}'.")
        else:
            output["parsing_errors"].append("Could not parse score for Response A.")

        # Explanation for Response B
        exp_b_match = re.search(
            r"Evaluation for Response B:\s*(.*?)\s*Score for Response B:",
            judge_response_text, re.DOTALL | re.IGNORECASE
        )
        if exp_b_match:
            output["response_B"]["explanation"] = exp_b_match.group(1).strip()
        else:
            output["parsing_errors"].append("Could not parse explanation for Response B.")

        # Score for Response B
        score_b_match = re.search(
            r"Score for Response B:\s*(\d+(?:\.\d+)?)(?:/\d+)?",
            judge_response_text, re.IGNORECASE
        )
        if score_b_match:
            try:
                output["response_B"]["score"] = float(score_b_match.group(1))
            except ValueError:
                output["parsing_errors"].append(f"Failed to parse score for Response B from '{score_b_match.group(1)}'.")
        else:
            output["parsing_errors"].append("Could not parse score for Response B.")
        
        # Final check if no structured info was found despite no specific regex error
        if not output["response_A"]["explanation"] and output["response_A"]["score"] is None and \
           not output["response_B"]["explanation"] and output["response_B"]["score"] is None and \
           not output["parsing_errors"]:
            output["parsing_errors"].append("Judge response did not match expected structure, but no specific regex pattern failed.")

    except Exception as e:
        # This catches errors in the regex execution itself or other unexpected errors
        output["parsing_errors"].append(f"Critical post-processing error: {str(e)}")

    return output


def evaluate_two_statements(
    input_dictionary: Dict[str, str],
    openai_config_path: str = "evaluator/openai_config.yaml", # Relative to VLM_DCT_Adapter
    judge_model_name: str = "gpt-4o",
    judge_system_prompt: Optional[str] = None,
    eval_output_dir: str = "./eval_results_two_statements"
) -> Dict[str, Any]:
    """
    Evaluates two text statements (response_A, response_B) against a query using an LLM judge.

    Args:
        input_dictionary: A dictionary with keys "query", "response_A", "response_B".
        openai_config_path: Path to the OpenAI configuration YAML file.
        judge_model_name: The name of the LLM judge model to use (e.g., "gpt-4o").
        judge_system_prompt: Optional system prompt for the judge.
        eval_output_dir: Directory where the GenericLLMEvaluator will save its detailed JSON output.

    Returns:
        A dictionary containing the original query, responses, and the judge's parsed evaluation
        (scores and explanations for each response), raw judge response, and any parsing errors.
    """
    if not all(k in input_dictionary for k in ["query", "response_A", "response_B"]):
        logger.error("Input dictionary must contain 'query', 'response_A', and 'response_B'.")
        return {
            "error": "Invalid input_dictionary structure.",
            "query": input_dictionary.get("query"),
            "response_A_text": input_dictionary.get("response_A"),
            "response_B_text": input_dictionary.get("response_B"),
        }

    os.makedirs(eval_output_dir, exist_ok=True)
    openai_params = load_openai_config(openai_config_path)

    # Configure the judge
    judge_cfg_dict = {
        "model": judge_model_name,
        "key": openai_params.get("api_key"),
        "openai_api_base": openai_params.get("base_url"),
        "temperature": 0.0,
        "max_out_len": 1024, # Judge might need more tokens for two explanations
        "query_per_second": 1, # Adjust as needed
        "system_prompt_content": judge_system_prompt
    }

    # Configure the prompt template
    prompt_template_config = ConfigDict(TWO_STATEMENT_PROMPT_TEMPLATE)
    judge_config = ConfigDict(judge_cfg_dict)

    # Path for GenericLLMEvaluator's detailed output file
    evaluator_internal_output_path = os.path.join(
        eval_output_dir,
        f"judge_details_{judge_model_name.replace('/', '_')}.jsonl"
    )

    # Initialize the GenericLLMEvaluator
    evaluator = GenericLLMEvaluator(
        judge_cfg=judge_config,
        prompt_template=prompt_template_config,
        dict_postprocessor=two_statement_postprocessor,
        output_path=evaluator_internal_output_path, # GenericLLMEvaluator will save its raw output here
    )
    
    data_for_dataset = {
        "query": [input_dictionary["query"]],
        "response_A": [input_dictionary["response_A"]],
        "response_B": [input_dictionary["response_B"]],
        "id": ["eval_0"] # Add a dummy ID
    }
    eval_hf_dataset = Dataset.from_dict(data_for_dataset)

    # The `predictions` argument for `evaluator.score()` is usually a list of single model outputs.
    # Since our prompt template gets 'response_A' and 'response_B' from the dataset columns,
    # we can pass a dummy list for `predictions` that matches the dataset length (which is 1).
    dummy_predictions = ["N/A"]

    logger.info(f"Sending request to judge model: {judge_model_name} for query: '{input_dictionary['query'][:50]}...'")
    
    evaluation_results = {}
    try:
        evaluation_results = evaluator.score(
            predictions=dummy_predictions,
            test_set=eval_hf_dataset
        )
    except Exception as e:
        logger.error(f"Error during evaluator.score: {e}", exc_info=True)
        return {
            "error": f"Evaluation failed: {str(e)}",
            "query": input_dictionary["query"],
            "response_A_text": input_dictionary["response_A"],
            "response_B_text": input_dictionary["response_B"],
        }

    # Process the results
    # `evaluation_results['details']` is a list of results, one for each item in test_set.
    # We have only one item.
    if evaluation_results and evaluation_results.get("details") and len(evaluation_results["details"]) > 0:
        judge_output_parsed = evaluation_results["details"][0].get("judge_evaluation", {})
        
        # Construct the final return dictionary
        final_output = {
            "query": input_dictionary["query"],
            "response_A_text": input_dictionary["response_A"],
            "response_B_text": input_dictionary["response_B"],
            "evaluation_A": judge_output_parsed.get("response_A", {"score": None, "explanation": "Not parsed"}),
            "evaluation_B": judge_output_parsed.get("response_B", {"score": None, "explanation": "Not parsed"}),
            "raw_judge_response": judge_output_parsed.get("raw_judge_response", "Not available"),
            "parsing_errors": judge_output_parsed.get("parsing_errors", ["No evaluation details found."])
        }
        # If parsing errors were significant, indicate it
        if not final_output["evaluation_A"]["explanation"] and not final_output["evaluation_A"]["score"] and \
           not final_output["evaluation_B"]["explanation"] and not final_output["evaluation_B"]["score"]:
            if "Judge response did not match expected structure, but no specific regex pattern failed." not in final_output["parsing_errors"] and \
               any("Critical post-processing error" not in err for err in final_output["parsing_errors"]):
                 final_output["parsing_errors"].append("Failed to extract structured scores or explanations for either response.")
        
        return final_output
    else:
        logger.error("Evaluation results were empty or not in the expected format.")
        return {
            "error": "Evaluation produced no details.",
            "query": input_dictionary["query"],
            "response_A_text": input_dictionary["response_A"],
            "response_B_text": input_dictionary["response_B"],
            "raw_evaluation_results": evaluation_results # include for debugging
        }

if __name__ == '__main__':
    openai_config_path = "evaluator/openai_config.yaml" # This path is relative to VLM_DCT_Adapter
    
    sample_input = {
        "query": "What are the pros and cons of remote work?",
        "response_A": "Pros: Flexibility, no commute. Cons: Isolation, home distractions.",
        "response_B": "Remote work offers enhanced work-life balance and reduced overhead for companies. However, it can lead to communication challenges and requires self-discipline."
    }

    logger.info(f"Evaluating sample input. Ensure '{openai_config_path}' contains valid OpenAI credentials.")
    
    temp_config = load_openai_config(openai_config_path)
    judgment = evaluate_two_statements(sample_input, openai_config_path=openai_config_path)
    logger.info("\n--- Evaluation Result ---")
    logger.info(json.dumps(judgment, indent=4))
    logger.info("--- End of Evaluation Result ---")

    # Example with a slightly more complex case for parsing
    sample_input_2 = {
        "query": "Explain quantum computing in simple terms.",
        "response_A": "It's like super-powered computers using qubits. Score for Response A: 8", # Score inside explanation
        "response_B": "Quantum computing leverages quantum mechanics to solve complex problems beyond classical computers. Evaluation for Response B: This is a good start but could be simpler. Score for Response B: 7/10"
    }
    
    logger.info(f"\nEvaluating second sample input (testing parser flexibility)...")
    judgment_2 = evaluate_two_statements(sample_input_2, openai_config_path=openai_config_path)
    logger.info("\n--- Evaluation Result 2 ---")
    logger.info(json.dumps(judgment_2, indent=4))
    logger.info("--- End of Evaluation Result 2 ---")

