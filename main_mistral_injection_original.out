Loading dataset from evaluator/benchmark_datasets/mtbench101.jsonl...
Dataset loaded. Training samples: 50, Evaluation samples: 50
Tokenizer pad_token was None, set to eos_token: </s>
--- Starting Evaluation for Original Model: mistralai/Mistral-7B-Instruct-v0.2 ---

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.28s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.25s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.19s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]
Error during original model evaluation: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 9.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 23.32 GiB is allocated by PyTorch, and 1.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
--- Starting Evaluation for Adapted Model: mistralai/Mistral-7B-Instruct-v0.2 + Adapter ---
Loading base model (mistralai/Mistral-7B-Instruct-v0.2) for adapter injection...

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.23s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.17s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.19s/it]
Model architecture written to ./eval_results_injection/model_archi.txt
Hello
Starting adapter injection with DCTAdapter...
Matched target layer for injection: model.layers.31.self_attn.q_proj
Dynamically setting adapter 'in_features' for model.layers.31.self_attn.q_proj to 4096 (derived from original_module.out_features).
Failed to inject adapter into model.layers.31.self_attn.q_proj: DCTAdapter.__init__() got an unexpected keyword argument 'in_features'
Matched target layer for injection: model.layers.31.self_attn.k_proj
Dynamically setting adapter 'in_features' for model.layers.31.self_attn.k_proj to 1024 (derived from original_module.out_features).
Failed to inject adapter into model.layers.31.self_attn.k_proj: DCTAdapter.__init__() got an unexpected keyword argument 'in_features'
Matched target layer for injection: model.layers.31.self_attn.v_proj
Dynamically setting adapter 'in_features' for model.layers.31.self_attn.v_proj to 1024 (derived from original_module.out_features).
Failed to inject adapter into model.layers.31.self_attn.v_proj: DCTAdapter.__init__() got an unexpected keyword argument 'in_features'
Matched target layer for injection: model.layers.31.self_attn.o_proj
Dynamically setting adapter 'in_features' for model.layers.31.self_attn.o_proj to 4096 (derived from original_module.out_features).
Failed to inject adapter into model.layers.31.self_attn.o_proj: DCTAdapter.__init__() got an unexpected keyword argument 'in_features'
Adapter injection process finished.
trainable params: 0 || all params: 7241732096 || trainable%: 0.0
ADAPTER MODEL ARCHITECTURE
MistralForCausalLM(
  (model): MistralModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): MistralMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): MistralRMSNorm((4096,), eps=1e-05)
    (rotary_emb): MistralRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Adapter training not requested (perform_adapter_training=False).
Evaluation of adapted model--------------------
Error during adapted model setup or evaluation: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 9.06 MiB is free. Including non-PyTorch memory, this process has 23.57 GiB memory in use. Of the allocated memory 23.32 GiB is allocated by PyTorch, and 1.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
--- Main Mistral Injection Script Finished ---
